{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tools for Big Data Analytics \u2013 Group 3","text":""},{"location":"#understanding-from-numerical-control-driven-machining-control","title":"Understanding from Numerical Control Driven Machining Control","text":"<p>Client / Product Owner: Prof. Joaqu\u00edn Ordieres Mer\u00e9 (j.ordieres@upm.es)  </p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#1-project-overview","title":"1. Project Overview \ud83d\udcd8","text":""},{"location":"#11-objectives","title":"1.1 Objectives \ud83c\udfaf","text":"<ul> <li>Detect operation periods and idle times automatically  </li> <li>Visualize sequences of operations and repeated patterns  </li> <li>Estimate energy consumption and timing per program </li> <li>Identify which variables are influenced by each operation  </li> <li>Generate contextual alerts (type and location)  </li> <li>Develop an interactive UI dashboard for data exploration  </li> <li>Document algorithms and architecture for reproducibility  </li> </ul>"},{"location":"#2-team-roles","title":"2. Team &amp; Roles \ud83d\udc65","text":"Role Name Responsibilities Product Owner Joaqu\u00edn Ordieres Defines business case &amp; priorities Scrum Master Johan Facilitates Scrum, syncs with PO, manages sprints &amp; backlog Architect / Tech Liaison Federico Defines architecture, ensures integration between teams Documentation Lead Maxime Maintains structure &amp; integrates team deliverables Data Engineers Tim, Laur\u00e8ne, Antonia ETL, data cleaning, feature extraction Analysts / Modelers Max, Rickard, Sjoerd Pattern detection, anomaly analysis, KPIs UI / Visualization Team Liz, Anna, Keivan Dashboard design, visualization, frontend logic"},{"location":"#3-workflow","title":"3. Workflow \ud83d\udd01","text":""},{"location":"#31-sprint-cadence","title":"3.1 Sprint Cadence \u23f1\ufe0f","text":"<ul> <li>Sprint length: 2 weeks  </li> <li>Planning: Mondays (45 min)  </li> <li>Review &amp; Demo: Fridays or before lecture (30\u201345 min)  </li> <li>Retrospective: End of sprint (20\u201330 min, on Teams/WhatsApp for now)</li> </ul>"},{"location":"#32-scrum-board","title":"3.2 Scrum Board \ud83d\udccb","text":"<p>Trello Dashboard: TBDA Gp 3 Scrum Board </p> <p>Each card represents a user story (US), task, or bug.  </p> <p>Board Structure: Backlog \u2192 Next Sprint \u2192 Sprint Backlog \u2192 Dev \u2192 Code Review \u2192 Testing \u2192 Done (Sprint X)</p>"},{"location":"#33-definition-of-done-dod","title":"3.3 Definition of Done (DoD) \u2705","text":"<ul> <li>Code runs locally &amp; passes tests  </li> <li>Peer review completed  </li> <li>Documentation updated  </li> <li>Card moved to \u201cDone (Sprint X)\u201d  </li> </ul>"},{"location":"#4-communication","title":"4. Communication \ud83d\udcac","text":"<p>Channel Purpose Teams / WhatsApp    Daily syncs, quick coordination Trello  Task tracking, sprint planning GitHub  Code + Docs + Reviews Google Drive    Reports &amp; shared files</p>"},{"location":"architecture/","title":"\ud83e\uddf1 Architectural Evolution \u2013 CNC Data Analytics System","text":""},{"location":"architecture/#preliminary-note","title":"\u26a0\ufe0f Preliminary Note","text":"<p>The following architectural proposals (Architectures A, B, and C) have been initially designed with the support of ChatGPT (OpenAI) as a reference framework for the project \u201cUnderstanding from Numerical Control Driven Machining Control.\u201d</p> <p>They represent conceptual drafts and working hypotheses, not final decisions. All elements \u2014 including component choices, data flows, and execution layers \u2014 must be reviewed, discussed, and validated by the entire project team before implementation. Any modification, simplification, or extension should be agreed upon collectively to ensure technical consistency and feasibility.</p>"},{"location":"architecture/#architecture-a-local-batch-pipeline","title":"\u2699\ufe0f Architecture A \u2014 \u201cLocal Batch Pipeline\u201d","text":""},{"location":"architecture/#concept","title":"\ud83d\udd39 Concept","text":"<p>A simple, local-first architecture for initial experimentation. All processing is performed on the user\u2019s PC, while data are stored on a remote PostgreSQL database. It enables the team to explore the dataset, understand machine behavior, and prototype the analytics pipeline.  </p> <p>This is the starting point for the project \u2014 low complexity, minimal infrastructure, and ideal for the early development phase.</p>"},{"location":"architecture/#main-components","title":"\ud83e\udde9 Main Components","text":"Layer Component Execution / Hosting Description Production plant CNC Machine Physical system Produces raw operational variables (axis position, spindle load, etc.) Cloud / Server PostgreSQL Database (raw data) Hosted on UPM / remote server Stores high-frequency machine signals Local PC Python ETL (pandas, SQL) Executed locally in Jupyter / Spyder Extracts and cleans data, performs simple aggregations Python Analytics Executed locally in Jupyter / Spyder Computes KPIs, operation time, energy per program, and alerts Streamlit Dashboard (UI) Executed locally with Streamlit (Python web server) Visualizes results and provides basic user interaction User Operator / Analyst Local access Explores data, applies filters, interprets results"},{"location":"architecture/#workflow","title":"\ud83d\udd04 Workflow","text":"<ol> <li>The CNC machine sends raw variables to the PostgreSQL database.  </li> <li>Local ETL scripts retrieve and clean the data.  </li> <li>Analytics modules compute machine indicators and alerts.  </li> <li>The dashboard presents results interactively.</li> </ol>"},{"location":"architecture/#advantages","title":"\u2705 Advantages","text":"<ul> <li>Very simple to deploy and maintain  </li> <li>Ideal for small datasets or offline analysis  </li> <li>Full transparency and control for debugging and learning  </li> </ul>"},{"location":"architecture/#limitations","title":"\u26a0\ufe0f Limitations","text":"<ul> <li>Query time increases with dataset size  </li> <li>Manual updates required (no automation)  </li> <li>No real-time feedback; purely batch operation  </li> </ul>"},{"location":"architecture/#adoption-timeline","title":"\ud83d\udd52 Adoption timeline","text":"<p>Used in Phase 1 (project start). Goal: validate the data model, ensure database connectivity, and develop the first analysis scripts.</p>"},{"location":"architecture/#architecture-diagram","title":"\ud83d\udcf7 Architecture Diagram","text":""},{"location":"architecture/#architecture-b-optimized-time-series-pipeline","title":"\u26a1 Architecture B \u2014 \u201cOptimized Time-Series Pipeline\u201d","text":""},{"location":"architecture/#concept_1","title":"\ud83d\udd39 Concept","text":"<p>An upgraded version focusing on performance and scalability. PostgreSQL is extended with TimescaleDB, enabling hypertables and continuous aggregation for fast time-based queries. This allows the system to handle larger volumes of CNC data with near-real-time responsiveness.</p>"},{"location":"architecture/#main-components_1","title":"\ud83e\udde9 Main Components","text":"Layer Component Execution / Hosting Description Production plant CNC Machine Physical source Streams operational data continuously Cloud / Server PostgreSQL (raw data) Hosted on remote server Stores unprocessed variables TimescaleDB Hosted on cloud server \u2013 automatic aggregation Extends PostgreSQL to manage time-series efficiently Local PC Python ETL Executed locally in Jupyter / Spyder Loads data, applies additional transformations and cleaning Python Analytics Executed locally in Jupyter / Spyder Performs KPI calculation, energy/time estimation, and alert generation Streamlit Dashboard (UI) Executed locally with Streamlit Visualizes aggregated data with time filters and metrics User Operator / Analyst Local or LAN access Uses the dashboard for insights and comparisons"},{"location":"architecture/#workflow_1","title":"\ud83d\udd04 Workflow","text":"<ol> <li>The CNC data are inserted into PostgreSQL (raw).  </li> <li>TimescaleDB automatically builds hypertables and continuous aggregates.  </li> <li>Local ETL scripts access pre-aggregated data instead of raw tables.  </li> <li>Analytics compute KPIs and alerts efficiently.  </li> <li>The dashboard displays up-to-date results.</li> </ol>"},{"location":"architecture/#advantages_1","title":"\u2705 Advantages","text":"<ul> <li>Fast queries thanks to TimescaleDB hypertables  </li> <li>Continuous aggregation handled automatically on the server  </li> <li>Scalable to millions of records  </li> <li>Compatible with the same local Python workflow  </li> </ul>"},{"location":"architecture/#limitations_1","title":"\u26a0\ufe0f Limitations","text":"<ul> <li>Requires server configuration for TimescaleDB  </li> <li>Still partially manual (batch updates)  </li> <li>No true real-time streaming yet  </li> </ul>"},{"location":"architecture/#adoption-timeline_1","title":"\ud83d\udd52 Adoption timeline","text":"<p>Used in Phase 2 (mid-project). Goal: optimize performance and manage higher data volumes once the prototype is validated.</p>"},{"location":"architecture/#architecture-diagram_1","title":"\ud83d\udcf7 Architecture Diagram","text":""},{"location":"architecture/#architecture-c-streaming-real-time-pipeline","title":"\ud83c\udf10 Architecture C \u2014 \u201cStreaming / Real-Time Pipeline\u201d","text":""},{"location":"architecture/#concept_2","title":"\ud83d\udd39 Concept","text":"<p>The most advanced version, enabling real-time monitoring and analytics. This architecture integrates Apache Kafka for data streaming and Spark Structured Streaming for on-the-fly processing. It combines real-time pipelines with long-term storage and visualization.</p>"},{"location":"architecture/#main-components_2","title":"\ud83e\udde9 Main Components","text":"Layer Component Execution / Hosting Description Production plant CNC Machine Physical source Continuously generates real-time signals Cloud / Processing layer Apache Kafka (stream) Hosted on cloud server \u2013 streaming message broker Receives live data, buffers, and distributes messages Spark Structured Streaming Executed on cloud server \u2013 real-time processing engine Processes Kafka streams, detects patterns and alerts, forwards results PostgreSQL (operational storage) Hosted on remote server Stores latest operational data for fast queries TimescaleDB (historical storage) Hosted on remote server Maintains long-term time-series logs and aggregates Local PC Python ETL Executed locally in Jupyter / Spyder Periodically integrates cloud data for additional analysis Python Analytics Executed locally in Jupyter / Spyder Further exploration and validation of real-time results Streamlit Dashboard (UI) Executed locally with Streamlit Displays live machine state, alerts, and KPIs in near real-time User Operator / Analyst Web access Monitors operations and system health interactively"},{"location":"architecture/#workflow_2","title":"\ud83d\udd04 Workflow","text":"<ol> <li>CNC sensors send live data streams to Apache Kafka.  </li> <li>Kafka buffers and forwards events to Spark Structured Streaming.  </li> <li>Spark aggregates and processes data in micro-batches, detecting operation cycles and anomalies.  </li> <li>Results are written to PostgreSQL (for fast querying) and TimescaleDB (for historical logs).  </li> <li>The Streamlit dashboard continuously updates with live KPIs and alerts.</li> </ol>"},{"location":"architecture/#advantages_2","title":"\u2705 Advantages","text":"<ul> <li>Real-time data ingestion and processing  </li> <li>Immediate alerting and visualization  </li> <li>Hybrid storage (short-term + long-term)  </li> <li>Scalable to industrial workloads  </li> </ul>"},{"location":"architecture/#limitations_2","title":"\u26a0\ufe0f Limitations","text":"<ul> <li>Higher infrastructure complexity  </li> <li>Requires Kafka and Spark deployment  </li> <li>Needs reliable network and cloud configuration  </li> </ul>"},{"location":"architecture/#adoption-timeline_2","title":"\ud83d\udd52 Adoption timeline","text":"<p>Used in Phase 3 (final stage). Goal: demonstrate real-time analytics capability, provide continuous monitoring, and connect the full data lifecycle (machine \u2192 cloud \u2192 user).</p>"},{"location":"architecture/#architecture-diagram_2","title":"\ud83d\udcf7 Architecture Diagram","text":""},{"location":"architecture/#overall-evolution-summary","title":"\ud83e\udded Overall Evolution Summary","text":"Stage Main Goal Key Technologies Deployment Level A \u2013 Local Batch Prototype, data understanding PostgreSQL, Python (pandas), Streamlit Local execution B \u2013 Time-Series Optimized Performance &amp; scalability TimescaleDB, Python (ETL/Analytics) Local + Cloud C \u2013 Streaming / Real-Time Real-time insights &amp; automation Kafka, Spark, PostgreSQL, TimescaleDB Cloud + Local UI"},{"location":"architecture/#progressive-implementation-strategy","title":"\ud83d\udd04 Progressive Implementation Strategy","text":"<ol> <li>Phase 1: Start with Architecture A \u2192 focus on ETL pipeline, KPIs, and dashboard.  </li> <li>Phase 2: Migrate to Architecture B \u2192 activate TimescaleDB and optimize queries.  </li> <li>Phase 3: Extend to Architecture C \u2192 integrate streaming for real-time feedback and alerts.</li> </ol> <p>This progressive approach ensures that: - The team learns the full data lifecycle step by step. - Each phase delivers a working, demonstrable system. - The final architecture (C) aligns with modern Industry 4.0 and IoT paradigms \u2014 turning raw CNC signals into real-time, actionable intelligence.</p>"},{"location":"overview/","title":"Documentation overview","text":""},{"location":"overview/#1-purpose","title":"1. Purpose","text":"<p>This folder contains (should contain) all technical and client-facing documentation for the project. The goal is to keep the documentation consistent and easy to navigate, across all subteams.</p>"},{"location":"overview/#2-structure","title":"2. Structure","text":"File / Folder Description overview.md You are here \u2014 index of all documentation files. architecture.md System design, data flow, and integration between backend, data, and UI backend.md Backend API endpoints, database connection details ui_visualization.md Frontend/dashboard structure, visualization logic, and dependencies. client/ Client-facing documentation: user manual, demo guide etc. sprint_summaries/ Sprint-by-sprint summaries and notes."},{"location":"overview/#3-documentation-guidelines","title":"3. Documentation Guidelines","text":"<p>To keep documentation consistent and easy to navigate, every subteam should:  </p> <ol> <li>Update documentation alongside code changes. </li> <li>When you finish a new feature, update the relevant <code>.md</code> file in this folder.  </li> <li>Follow consistent formatting. </li> <li>Use clear section headers (<code>##</code>), bullet points, and short paragraphs.   </li> <li>Avoid duplication. </li> <li>Link to other files instead of rewriting the same content. Example:      See architecture.md </li> </ol>"}]}