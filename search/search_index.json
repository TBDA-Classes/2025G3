{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tools for Big Data Analytics \u2013 Group 3","text":""},{"location":"#understanding-from-numerical-control-driven-machining-control","title":"Understanding from Numerical Control Driven Machining Control","text":"<p>Client / Product Owner: Prof. Joaqu\u00edn Ordieres Mer\u00e9 (j.ordieres@upm.es)  </p>"},{"location":"#documentation-available-at","title":"Documentation available at:","text":"<p>https://tbda-classes.github.io/2025G3/</p>"},{"location":"#1-project-overview","title":"1. Project Overview \ud83d\udcd8","text":""},{"location":"#11-objectives","title":"1.1 Objectives \ud83c\udfaf","text":"<ul> <li>Detect operation periods and idle times automatically  </li> <li>Visualize sequences of operations and repeated patterns  </li> <li>Estimate energy consumption and timing per program </li> <li>Identify which variables are influenced by each operation  </li> <li>Generate contextual alerts (type and location)  </li> <li>Develop an interactive UI dashboard for data exploration  </li> <li>Document algorithms and architecture for reproducibility    </li> </ul>"},{"location":"#2-team-roles","title":"2. Team &amp; Roles \ud83d\udc65","text":"Role Name Responsibilities Product Owner Joaqu\u00edn Ordieres Defines business case &amp; priorities Scrum Master Johan Facilitates Scrum, syncs with PO, manages sprints &amp; backlog Architect/Documentation Federico Defines architecture, ensures integration between teams Documentation Maxime Maintains structure &amp; integrates team deliverables Backend + Extraction Tim, Laur\u00e8ne, Antonia Max, Rickard, Sjoerd Pattern detection, anomaly analysis, KPIs ETL, data cleaning, feature extraction UI / Visualization Team Liz, Anna, Keivan Dashboard design, visualization, frontend logic"},{"location":"#3-workflow","title":"3. Workflow","text":""},{"location":"#31-sprint-cadence","title":"3.1 Sprint Cadence","text":"<ul> <li>Sprint length: 2 weeks  </li> <li>Retrospective: End of sprint (10 min, at lecture, with the ones who aren't absent)</li> </ul>"},{"location":"#32-scrum-board","title":"3.2 Scrum Board \ud83d\udccb","text":"<p>ClickUp Dashboard (Invite via email needed): TBDA G3 Board</p> <p>(ARCHIVED because of 10 member limitation) Trello Dashboard: TBDA Gp 3 Scrum Board </p> <p>Each card represents a user story (US), task, or bug.  </p> <p>Board Structure: Backlog \u2192 Next Sprint \u2192 Sprint Backlog \u2192 Dev \u2192 Code Review \u2192 Testing \u2192 Done (Sprint X)</p>"},{"location":"#33-definition-of-done-dod","title":"3.3 Definition of Done (DoD) \u2705","text":"<ul> <li>Code runs locally &amp; passes tests  </li> <li>Peer review completed  </li> <li>Documentation updated  </li> <li>Card moved to \u201cDone (Sprint X)\u201d  </li> </ul>"},{"location":"#4-communication","title":"4. Communication","text":"<ul> <li>WhatsApp:  Quick coordination</li> <li>Trello:   Task tracking, sprint planning</li> <li>GitHub:   Code + Docs + Reviews</li> <li>Google Drive: Reports &amp; shared files</li> </ul>"},{"location":"architecture/","title":"\ud83e\uddf1 Architectural Evolution \u2013 CNC Data Analytics System","text":""},{"location":"architecture/#preliminary-note","title":"\u26a0\ufe0f Preliminary Note","text":"<p>The following architectural proposals (Architectures A, B, and C) have been initially designed with the support of ChatGPT (OpenAI) as a reference framework for the project \u201cUnderstanding from Numerical Control Driven Machining Control.\u201d</p> <p>They represent conceptual drafts and working hypotheses, not final decisions. All elements \u2014 including component choices, data flows, and execution layers \u2014 must be reviewed, discussed, and validated by the entire project team before implementation. Any modification, simplification, or extension should be agreed upon collectively to ensure technical consistency and feasibility.</p>"},{"location":"architecture/#architecture-a-local-batch-pipeline","title":"\u2699\ufe0f Architecture A \u2014 \u201cLocal Batch Pipeline\u201d","text":""},{"location":"architecture/#concept","title":"\ud83d\udd39 Concept","text":"<p>A simple, local-first architecture for initial experimentation. All processing is performed on the user\u2019s PC, while data are stored on a remote PostgreSQL database. It enables the team to explore the dataset, understand machine behavior, and prototype the analytics pipeline.  </p> <p>This is the starting point for the project \u2014 low complexity, minimal infrastructure, and ideal for the early development phase.</p>"},{"location":"architecture/#main-components","title":"\ud83e\udde9 Main Components","text":"Layer Component Execution / Hosting Description Production plant CNC Machine Physical system Produces raw operational variables (axis position, spindle load, etc.) Cloud / Server PostgreSQL Database (raw data) Hosted on UPM / remote server Stores high-frequency machine signals Local PC Python ETL (pandas, SQL) Executed locally in Jupyter / Spyder Extracts and cleans data, performs simple aggregations Python Analytics Executed locally in Jupyter / Spyder Computes KPIs, operation time, energy per program, and alerts Streamlit Dashboard (UI) Executed locally with Streamlit (Python web server) Visualizes results and provides basic user interaction User Operator / Analyst Local access Explores data, applies filters, interprets results"},{"location":"architecture/#workflow","title":"\ud83d\udd04 Workflow","text":"<ol> <li>The CNC machine sends raw variables to the PostgreSQL database.  </li> <li>Local ETL scripts retrieve and clean the data.  </li> <li>Analytics modules compute machine indicators and alerts.  </li> <li>The dashboard presents results interactively.</li> </ol>"},{"location":"architecture/#advantages","title":"\u2705 Advantages","text":"<ul> <li>Very simple to deploy and maintain  </li> <li>Ideal for small datasets or offline analysis  </li> <li>Full transparency and control for debugging and learning  </li> </ul>"},{"location":"architecture/#limitations","title":"\u26a0\ufe0f Limitations","text":"<ul> <li>Query time increases with dataset size  </li> <li>Manual updates required (no automation)  </li> <li>No real-time feedback; purely batch operation  </li> </ul>"},{"location":"architecture/#adoption-timeline","title":"\ud83d\udd52 Adoption timeline","text":"<p>Used in Phase 1 (project start). Goal: validate the data model, ensure database connectivity, and develop the first analysis scripts.</p>"},{"location":"architecture/#architecture-diagram","title":"\ud83d\udcf7 Architecture Diagram","text":""},{"location":"architecture/#architecture-b-optimized-time-series-pipeline","title":"\u26a1 Architecture B \u2014 \u201cOptimized Time-Series Pipeline\u201d","text":""},{"location":"architecture/#concept_1","title":"\ud83d\udd39 Concept","text":"<p>An upgraded version focusing on performance and scalability. PostgreSQL is extended with TimescaleDB, enabling hypertables and continuous aggregation for fast time-based queries. This allows the system to handle larger volumes of CNC data with near-real-time responsiveness.</p>"},{"location":"architecture/#main-components_1","title":"\ud83e\udde9 Main Components","text":"Layer Component Execution / Hosting Description Production plant CNC Machine Physical source Streams operational data continuously Cloud / Server PostgreSQL (raw data) Hosted on remote server Stores unprocessed variables TimescaleDB Hosted on cloud server \u2013 automatic aggregation Extends PostgreSQL to manage time-series efficiently Local PC Python ETL Executed locally in Jupyter / Spyder Loads data, applies additional transformations and cleaning Python Analytics Executed locally in Jupyter / Spyder Performs KPI calculation, energy/time estimation, and alert generation Streamlit Dashboard (UI) Executed locally with Streamlit Visualizes aggregated data with time filters and metrics User Operator / Analyst Local or LAN access Uses the dashboard for insights and comparisons"},{"location":"architecture/#workflow_1","title":"\ud83d\udd04 Workflow","text":"<ol> <li>The CNC data are inserted into PostgreSQL (raw).  </li> <li>TimescaleDB automatically builds hypertables and continuous aggregates.  </li> <li>Local ETL scripts access pre-aggregated data instead of raw tables.  </li> <li>Analytics compute KPIs and alerts efficiently.  </li> <li>The dashboard displays up-to-date results.</li> </ol>"},{"location":"architecture/#advantages_1","title":"\u2705 Advantages","text":"<ul> <li>Fast queries thanks to TimescaleDB hypertables  </li> <li>Continuous aggregation handled automatically on the server  </li> <li>Scalable to millions of records  </li> <li>Compatible with the same local Python workflow  </li> </ul>"},{"location":"architecture/#limitations_1","title":"\u26a0\ufe0f Limitations","text":"<ul> <li>Requires server configuration for TimescaleDB  </li> <li>Still partially manual (batch updates)  </li> <li>No true real-time streaming yet  </li> </ul>"},{"location":"architecture/#adoption-timeline_1","title":"\ud83d\udd52 Adoption timeline","text":"<p>Used in Phase 2 (mid-project). Goal: optimize performance and manage higher data volumes once the prototype is validated.</p>"},{"location":"architecture/#architecture-diagram_1","title":"\ud83d\udcf7 Architecture Diagram","text":""},{"location":"architecture/#architecture-c-streaming-real-time-pipeline","title":"\ud83c\udf10 Architecture C \u2014 \u201cStreaming / Real-Time Pipeline\u201d","text":""},{"location":"architecture/#concept_2","title":"\ud83d\udd39 Concept","text":"<p>The most advanced version, enabling real-time monitoring and analytics. This architecture integrates Apache Kafka for data streaming and Spark Structured Streaming for on-the-fly processing. It combines real-time pipelines with long-term storage and visualization.</p>"},{"location":"architecture/#main-components_2","title":"\ud83e\udde9 Main Components","text":"Layer Component Execution / Hosting Description Production plant CNC Machine Physical source Continuously generates real-time signals Cloud / Processing layer Apache Kafka (stream) Hosted on cloud server \u2013 streaming message broker Receives live data, buffers, and distributes messages Spark Structured Streaming Executed on cloud server \u2013 real-time processing engine Processes Kafka streams, detects patterns and alerts, forwards results PostgreSQL (operational storage) Hosted on remote server Stores latest operational data for fast queries TimescaleDB (historical storage) Hosted on remote server Maintains long-term time-series logs and aggregates Local PC Python ETL Executed locally in Jupyter / Spyder Periodically integrates cloud data for additional analysis Python Analytics Executed locally in Jupyter / Spyder Further exploration and validation of real-time results Streamlit Dashboard (UI) Executed locally with Streamlit Displays live machine state, alerts, and KPIs in near real-time User Operator / Analyst Web access Monitors operations and system health interactively"},{"location":"architecture/#workflow_2","title":"\ud83d\udd04 Workflow","text":"<ol> <li>CNC sensors send live data streams to Apache Kafka.  </li> <li>Kafka buffers and forwards events to Spark Structured Streaming.  </li> <li>Spark aggregates and processes data in micro-batches, detecting operation cycles and anomalies.  </li> <li>Results are written to PostgreSQL (for fast querying) and TimescaleDB (for historical logs).  </li> <li>The Streamlit dashboard continuously updates with live KPIs and alerts.</li> </ol>"},{"location":"architecture/#advantages_2","title":"\u2705 Advantages","text":"<ul> <li>Real-time data ingestion and processing  </li> <li>Immediate alerting and visualization  </li> <li>Hybrid storage (short-term + long-term)  </li> <li>Scalable to industrial workloads  </li> </ul>"},{"location":"architecture/#limitations_2","title":"\u26a0\ufe0f Limitations","text":"<ul> <li>Higher infrastructure complexity  </li> <li>Requires Kafka and Spark deployment  </li> <li>Needs reliable network and cloud configuration  </li> </ul>"},{"location":"architecture/#adoption-timeline_2","title":"\ud83d\udd52 Adoption timeline","text":"<p>Used in Phase 3 (final stage). Goal: demonstrate real-time analytics capability, provide continuous monitoring, and connect the full data lifecycle (machine \u2192 cloud \u2192 user).</p>"},{"location":"architecture/#architecture-diagram_2","title":"\ud83d\udcf7 Architecture Diagram","text":""},{"location":"architecture/#overall-evolution-summary","title":"\ud83e\udded Overall Evolution Summary","text":"Stage Main Goal Key Technologies Deployment Level A \u2013 Local Batch Prototype, data understanding PostgreSQL, Python (pandas), Streamlit Local execution B \u2013 Time-Series Optimized Performance &amp; scalability TimescaleDB, Python (ETL/Analytics) Local + Cloud C \u2013 Streaming / Real-Time Real-time insights &amp; automation Kafka, Spark, PostgreSQL, TimescaleDB Cloud + Local UI"},{"location":"architecture/#progressive-implementation-strategy","title":"\ud83d\udd04 Progressive Implementation Strategy","text":"<ol> <li>Phase 1: Start with Architecture A \u2192 focus on ETL pipeline, KPIs, and dashboard.  </li> <li>Phase 2: Migrate to Architecture B \u2192 activate TimescaleDB and optimize queries.  </li> <li>Phase 3: Extend to Architecture C \u2192 integrate streaming for real-time feedback and alerts.</li> </ol> <p>This progressive approach ensures that: - The team learns the full data lifecycle step by step. - Each phase delivers a working, demonstrable system. - The final architecture (C) aligns with modern Industry 4.0 and IoT paradigms \u2014 turning raw CNC signals into real-time, actionable intelligence.</p>"},{"location":"overview/","title":"Documentation overview","text":""},{"location":"overview/#1-purpose","title":"1. Purpose","text":"<p>This folder contains all technical and client-facing documentation for the project. The goal is to keep the documentation consistent and easy to navigate across all subteams.</p>"},{"location":"overview/#2-structure","title":"2. Structure","text":"File / Folder Description index.md Landing page for the documentation site. overview.md You are here \u2013 global index of all documentation files. architecture.md System design, data flow, and integration between extraction, backend, and UI. ui_visualization.md Overall dashboard / visualization concept and UX decisions. backend/ Backend documentation: API endpoints, service layer, and DB connection. backend/index.md High-level description of the backend and how components fit together. backend/api.md API reference for the FastAPI endpoints (generated with mkdocstrings). backend/services.md Reference for the database / aggregation functions (generated with mkdocstrings). extraction/ Documentation for data extraction and preprocessing. extraction/data_analysis.md Detailed description of the current extraction and analysis pipeline. frontend/ui.md Description of the frontend dashboard, main screens and components. client/user_manual.md Client-facing user manual and demo instructions. sprint-summaries/ Sprint-by-sprint summaries and notes."},{"location":"overview/#3-documentation-guidelines","title":"3. Documentation guidelines","text":"<p>To keep documentation consistent and easy to navigate, every subteam should:</p> <ol> <li> <p>Update documentation alongside code changes.    When you finish a new feature, update the relevant <code>.md</code> file in this folder.</p> </li> <li> <p>Follow consistent formatting.    Use clear section headers (<code>##</code>), bullet points, and short paragraphs.</p> </li> <li> <p>Avoid duplication.    Link to other files instead of rewriting the same content. Example:    See architecture.md for overall system design.</p> </li> </ol>"},{"location":"backend/","title":"Backend overview","text":"<p>The backend is a FastAPI application that exposes a small read-only API on top of the CNC data stored in PostgreSQL.</p> <ul> <li>API endpoints describe the HTTP interface.</li> <li>Services describe the query and aggregation logic.</li> </ul>"},{"location":"backend/api/","title":"Backend API","text":""},{"location":"backend/api/#how-to-run-the-backend","title":"How to run the backend","text":"<p>From the project root:</p> <pre><code>cd backend\nsource .venv/bin/activate\ncd ..\nuvicorn backend.main:app --reload\n</code></pre>"},{"location":"backend/api/#fastapi-endpoints","title":"FastAPI endpoints","text":""},{"location":"backend/api/#backend.main.get_daily_temp_avg","title":"<code>get_daily_temp_avg(date=Query(...))</code>","text":"<p>Return the average daily temperature for a given date.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>str</code> <p>ISO date string YYYY-MM-DD.</p> <code>Query(...)</code> <p>Returns:</p> Type Description <p>A dict with the date and average temperature value.</p>"},{"location":"backend/api/#backend.main.get_daily_alerts_number","title":"<code>get_daily_alerts_number(date=Query(...))</code>","text":"<p>Return the number of alert snapshots recorded on the given date.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>str</code> <p>ISO date string YYYY-MM-DD.</p> <code>Query(...)</code> <p>Returns:</p> Type Description <p>Dict with key 'num_alarms'.</p>"},{"location":"backend/api/#backend.main.get_critical_alerts_data","title":"<code>get_critical_alerts_data(date=Query(...))</code>","text":"<p>Return all critical alert events for the given date.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>str</code> <p>ISO date string YYYY-MM-DD.</p> <code>Query(...)</code> <p>Returns:</p> Type Description <p>List of dicts with timestamp and alert description.</p>"},{"location":"backend/api/#backend.main.get_daily_spindle_avg","title":"<code>get_daily_spindle_avg(date=Query(...))</code>","text":"<p>Return the average daily spindle load for the given date.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>str</code> <p>ISO date string YYYY-MM-DD.</p> <code>Query(...)</code> <p>Returns:</p> Type Description <p>Dict with average spindle load.</p>"},{"location":"backend/api/#backend.main.get_hourly_spindle_avg","title":"<code>get_hourly_spindle_avg(date=Query(...))</code>","text":"<p>Return hourly average spindle load for the given date.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>str</code> <p>ISO date string YYYY-MM-DD.</p> <code>Query(...)</code> <p>Returns:</p> Type Description <p>List of hourly averages.</p>"},{"location":"backend/api/#backend.main.get_hourly_temp_avg","title":"<code>get_hourly_temp_avg(date=Query(...))</code>","text":"<p>Return hourly average temperature for the given date.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>str</code> <p>ISO date string YYYY-MM-DD.</p> <code>Query(...)</code> <p>Returns:</p> Type Description <p>List of hourly averages.</p>"},{"location":"backend/api/#backend.main.get_hourly_combined","title":"<code>get_hourly_combined(date=Query(...))</code>","text":"<p>Return hourly averages of both temperature and spindle load.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>str</code> <p>ISO date string YYYY-MM-DD.</p> <code>Query(...)</code> <p>Returns:</p> Type Description <p>List of dicts with hour, avg_temp, and avg_spindle.</p>"},{"location":"backend/services/","title":"Service layer","text":"<p>These functions implement the database queries and aggregations used by the FastAPI endpoints.</p>"},{"location":"backend/services/#backend.services.get_critical_alerts","title":"<code>get_critical_alerts(db_conn, date)</code>","text":"<p>Fetch all critical alerts for the specified date.</p> <p>Parameters:</p> Name Type Description Default <code>db_conn</code> <p>PostgreSQL connection.</p> required <code>date</code> <p>ISO date string YYYY-MM-DD.</p> required <p>Returns:</p> Type Description <p>List of dicts with timestamp and alert description.</p>"},{"location":"backend/services/#backend.services.get_daily_average_power","title":"<code>get_daily_average_power(db_conn, date_str)</code>","text":"<p>Ber\u00e4knar genomsnittlig effekt (kW) f\u00f6r hela dygnet.</p>"},{"location":"backend/services/#backend.services.get_daily_average_spindle_load","title":"<code>get_daily_average_spindle_load(db_conn, date)</code>","text":"<p>Compute the average spindle load for a single day.</p> <p>Parameters:</p> Name Type Description Default <code>db_conn</code> <p>PostgreSQL connection.</p> required <code>date</code> <p>ISO date string YYYY-MM-DD.</p> required <p>Returns:</p> Type Description <p>Dict with log_time and avg_spindle.</p>"},{"location":"backend/services/#backend.services.get_daily_average_temp","title":"<code>get_daily_average_temp(db_conn, date)</code>","text":"<p>Compute the average temperature for a single day.</p> <p>Parameters:</p> Name Type Description Default <code>db_conn</code> <p>PostgreSQL connection.</p> required <code>date</code> <p>ISO date string YYYY-MM-DD.</p> required <p>Returns:</p> Type Description <p>Dict with log_time and avg_temp.</p>"},{"location":"backend/services/#backend.services.get_hourly_combined_stats","title":"<code>get_hourly_combined_stats(db_conn, date_str)</code>","text":"<p>Return hourly averages of temperature, spindle load, AND power usage.</p> <p>Parameters:</p> Name Type Description Default <code>db_conn</code> <p>PostgreSQL connection.</p> required <code>date_str</code> <p>ISO date string YYYY-MM-DD.</p> required <p>Returns:</p> Type Description <p>List of dicts with log_hour, avg_temp, avg_spindle, and power_kW.</p>"},{"location":"backend/services/#backend.services.get_number_daily_alerts","title":"<code>get_number_daily_alerts(db_conn, date)</code>","text":"<p>Count how many alert snapshots exist for the date.</p> <p>Parameters:</p> Name Type Description Default <code>db_conn</code> <p>PostgreSQL connection.</p> required <code>date</code> <p>ISO date string YYYY-MM-DD.</p> required <p>Returns:</p> Type Description <p>Dict containing the number of alerts.</p>"},{"location":"client/user_guide/","title":"User Guide \u2013 CNC Analytics Dashboard","text":"<p>This document explains how to run and use the CNC Analytics Dashboard locally. It is intended for technical users and evaluators of the project.</p> <p>The system consists of: - A FastAPI backend that exposes CNC data through a REST API - A React frontend that visualizes the data in a dashboard</p>"},{"location":"client/user_guide/#both-components-must-be-running-locally-for-the-dashboard-to-function","title":"Both components must be running locally for the dashboard to function.","text":""},{"location":"client/user_guide/#1-system-overview","title":"1. System Overview","text":"<p>The dashboard allows users to: - Select a date - View daily average KPIs:   - Temperature (\u00b0C)   - Spindle load (%)   - Power usage (kW)   - Number of alerts - Inspect hourly trends for temperature, spindle load, and power - View critical alert for the selected day</p> <pre><code>Only dates with available historical data (typically before 2022) will return results.\n- **An example of a week with data is one starting on 2021-11-22**\n</code></pre>"},{"location":"client/user_guide/#2-dashboard-usage","title":"2. Dashboard Usage","text":""},{"location":"client/user_guide/#selecting-a-date","title":"Selecting a Date","text":"<ol> <li>Use the date picker at the top of the dashboard</li> <li>Select a date with available data</li> <li>Click Apply</li> </ol> <p>All KPIs, charts, and alerts update automatically.</p>"},{"location":"client/user_guide/#kpi-cards","title":"KPI Cards","text":"<p>The top cards show: - Average Daily Temperature (\u00b0C) - Average Daily Spindle Load (%) - Daily Alerts (count) - Average Daily Power (kW)</p>"},{"location":"client/user_guide/#hourly-performance-chart","title":"Hourly Performance Chart","text":"<p>The chart displays hourly averages for: - Temperature (\u00b0C) - Spindle load (%) - Power usage (kW)</p> <p>Each metric has its own axis and color.</p>"},{"location":"client/user_guide/#critical-alerts-panel","title":"Critical Alerts Panel","text":"<p>The right-hand panel lists critical alerts: - Timestamp - Alert description (e.g. EMERGENCIA EXTERNA)</p>"},{"location":"client/user_guide/#3-repository-structure-relevant-parts","title":"3. Repository Structure (relevant parts)","text":"<p>From the project root:</p> <p><code>2025G3/   backend/            # FastAPI backend   frontend/TBDA/      # React frontend   docs/               # Project documentation</code></p>"},{"location":"client/user_guide/#4-prerequisites","title":"4. Prerequisites","text":"<p>To run the system locally, you need:</p> <ul> <li>Python 3.11 or newer</li> <li>Node.js and npm</li> <li>Access to the course PostgreSQL database</li> <li>A .env file with database credentials</li> </ul>"},{"location":"client/user_guide/#5-5-running-the-backend-fastapi","title":"5. 5. Running the Backend (FastAPI)","text":"<p>Activate the virtual environment From the project root:</p> <p><code>cd backend source .venv/bin/activate cd .. uvicorn backend.main:app --reload</code></p> <p>If successful, you should see output similar to: <code>Uvicorn running on http://127.0.0.1:8000</code></p> <p>The backend API will now be available at: http://localhost:8000</p>"},{"location":"extraction/data_analysis/","title":"Extraction / Data analysis  Overview","text":"<p>This branch is for the extraction and analyst team to be able to extract the data with a code that does not affect the backend/frontend.  This so the team can analyse and see different data which will be important for the decision of data for the different KPIs and such.</p>"},{"location":"extraction/data_analysis/#what-this-module-does","title":"What this module does","text":"<ul> <li>Exploring data in the database</li> <li>Make queeries for different variables and datasets to explore them</li> <li>Analyse different data and there frequenzies</li> </ul>"},{"location":"extraction/data_analysis/#what-has-been-done","title":"What has been done","text":"<ul> <li>The extraction group have checked the database and looked through all the ID.s that are interesting for the client to be seen. These have later been translated from the unit ids to variable ids. An exceldocument was after this created so the backend and frontend teams could use the right IDs. Discussions in the the whole team has also unfolded where talks about which parameters to use. </li> <li>Code for extraction with different queries have been developed and used for the extraction.</li> </ul>"},{"location":"extraction/data_analysis/#2411-25","title":"24/11-25","text":"<ul> <li> <p>A code for extraction of system status as on/off has been developed where the querie counts all the variables that are active every hour during set dates. By looking at when the variables are not active, we can see when the machine is on or off. This is done with the following querie that looks at all the variables in variable_log_float instead of specific IDs.</p> </li> <li> <p>q = text(\"\"\"     SELECT         DATE(to_timestamp(date/1000)) AS day,         EXTRACT(HOUR FROM to_timestamp(date/1000)) AS hour,         COUNT(*) AS log_count     FROM public.variable_log_float     WHERE date BETWEEN :start_ms AND :end_ms     GROUP BY day, hour     ORDER BY day, hour; \"\"\")</p> </li> </ul> <p>This is the result for a five day period in january in 2015.</p> <p></p> <p></p> <p>The full code can be seen in the extraction folder under system_status.py </p>"},{"location":"extraction/data_analysis/#0312-25","title":"03/12-25","text":"<ul> <li> <p>Another code for extraction of system status as on/off/Idle has been developed that instead of counting all the variables that are active every hour uses the variable_id 597 which is system_in_progess. This means that when the system is of, it returns nodata. When it is in idle it returns zero and when it is on it returns a one. This is then shown in graph that shows colors with red for off, yellow for idle and green for on.</p> </li> <li> <p>The querie for the extraction is as follows:   q = text(\"\"\"     SELECT to_timestamp(date/1000) AS real_date, value     FROM public.variable_log_float     WHERE id_var = :vid       AND date BETWEEN :start_ms AND :end_ms     ORDER BY date; \"\"\") </p> </li> </ul> <p></p>"},{"location":"extraction/data_analysis/#code-reference","title":"Code Reference","text":""},{"location":"frontend/ui/","title":"Frontend overview","text":"<p>The frontend is a React-based single-page application that lets users explore machine data for a selected day.</p>"},{"location":"frontend/ui/#tech-stack","title":"Tech stack","text":"<ul> <li>React + TypeScript (Vite)</li> <li>HTTP client: fetch / axios (depending on what you actually use)</li> <li>Deployed as static assets (served separately from the FastAPI backend)</li> </ul>"},{"location":"frontend/ui/#main-responsibilities","title":"Main responsibilities","text":"<ul> <li>Let the user pick a date.</li> <li>Call the backend API endpoints:</li> <li><code>/api/daily_temp_avg</code></li> <li><code>/api/daily_spindle_avg</code></li> <li><code>/api/hourly_temp_avg</code></li> <li><code>/api/hourly_spindle_avg</code></li> <li><code>/api/hourly_combined</code></li> <li><code>/api/critical_alerts</code></li> <li>Render:</li> <li>daily KPIs (cards)</li> <li>hourly graphs (temperature, spindle load, later energy)</li> <li>list of alerts for the selected day</li> </ul>"},{"location":"frontend/ui_visualization/","title":"UI / Visualization design","text":"<p>This page documents the UX drafts, mockups, and design evolution of the dashboard.</p>"},{"location":"frontend/ui_visualization/#ux-draft-presented-on-311","title":"UX Draft (presented on 3/11)","text":""},{"location":"frontend/ui_visualization/#user-and-goals","title":"User and goals","text":"<p>Users: - Machine operators and engineers - Analysts monitoring performance and energy use</p> <p>Their goals: - Identify when the machine is operating, idle, or in error - Visualize KPIs like energy per program, cycle duration, spindle load - See alerts or anomalies in real time</p>"},{"location":"frontend/ui_visualization/#key-information-to-show","title":"Key information to show","text":"<ul> <li>Machine state: operating / standby / emergency / stopped</li> <li>Program info: name, progress, tool used</li> <li>Performance metrics: spindle load, temperature, feedrate, operation time</li> <li>Alerts: overheat, emergency, vibration</li> <li>Energy: estimated energy per program</li> </ul>"},{"location":"frontend/ui_visualization/#ux-draft","title":"UX draft","text":"<p>Updated Figma draft</p>"},{"location":"frontend/ui_visualization/#feedback-from-the-professor","title":"Feedback from the professor","text":"<p>We don\u2019t have access to current data, so we won\u2019t be able to show live signals. More interesting is to be able to go back in time and see which programs and data points occurred during certain dates.</p>"},{"location":"frontend/ui_visualization/#next-steps-and-goals","title":"Next steps and goals","text":"<p>We\u2019re now beginning the development of the website. Its main purpose is to let users select a past date and view a dashboard displaying machine-operation data for that specific day. For the time being, the daily data we intend to present includes:</p> <ul> <li>Average daily temperature</li> <li>Average daily spindle load</li> <li>Energy consumed</li> <li>Graphs showing temperature, spindle load and energy consumption throughout   the day</li> <li>Alerts (if applicable)</li> </ul>"},{"location":"frontend/ui_visualization/#first-draft-of-the-website-presented-2411","title":"First draft of the website (presented 24/11)","text":""},{"location":"frontend/ui_visualization/#website-draft","title":"Website draft","text":""},{"location":"frontend/ui_visualization/#progress","title":"Progress","text":"<p>We have successfully established a connection between the client and the server which is connected to the database. On the dashboard, we have currently implemented the following:</p> <ul> <li>Average daily temperature</li> <li>Average daily spindle load</li> </ul>"},{"location":"frontend/ui_visualization/#next-steps","title":"Next steps","text":"<p>Our immediate focus is to include alerts on the dashboard. Following this, we will implement the energy-consumption metric. If time and resources permit, we will proceed with developing the visualization graphs for these measurements.</p>"},{"location":"sprint-summaries/sprint1/","title":"Sprint 1 Summary (30/09 \u2013 13/10)","text":""},{"location":"sprint-summaries/sprint1/#sprint-wrap-up","title":"\ud83c\udfc1 Sprint Wrap-up","text":""},{"location":"sprint-summaries/sprint1/#what-has-been-done","title":"What has been done","text":"<ul> <li>Project team organized according to Scrum methodology (roles, subteams, tools defined).</li> <li>Repository and ClickUp workspace created and configured.</li> <li>Initial meetings with the client to understand goals and available data.</li> <li>First exploration of datasets shared by the client.</li> <li>Project architecture brainstorming and first drafts prepared.</li> </ul>"},{"location":"sprint-summaries/sprint1/#what-went-well","title":"What went well","text":"<ul> <li>Team adapted to collaborative tools (GitHub, ClickUp)</li> </ul>"},{"location":"sprint-summaries/sprint1/#what-can-we-do-better","title":"What can we do better","text":"<ul> <li>Improve documentation frequency.</li> <li>Maintain updated ClickUp tasks to track progress more effectively.</li> <li>Ensure all members are aligned on the data understanding phase.</li> </ul>"},{"location":"sprint-summaries/sprint1/#next-sprint-1310-2610","title":"\ud83d\ude80 Next Sprint (13/10 \u2013 26/10)","text":""},{"location":"sprint-summaries/sprint1/#what-should-be-done","title":"What should be done","text":"<ul> <li>Backend connection to the database.</li> <li>Frontend starts interacting with API endpoints.</li> <li>Define and evaluate alternative architecture proposals.</li> <li>Continue dataset cleaning and understanding.</li> </ul>"},{"location":"sprint-summaries/sprint1/#documentation","title":"Documentation","text":"<ul> <li>Create dedicated <code>.md</code> files for backend, frontend, and data analysis.</li> <li>Upload diagrams or drafts related to the three architecture proposals.</li> </ul>"},{"location":"sprint-summaries/sprint1/#user-stories","title":"User Stories","text":"<ul> <li>Refine initial user stories and link them to corresponding tasks in ClickUp.</li> <li>Define clear acceptance criteria for technical and documentation deliverables.</li> </ul>"},{"location":"sprint-summaries/sprint2/","title":"Sprint 2 Summary (13/10 \u2013 26/10)","text":""},{"location":"sprint-summaries/sprint2/#sprint-wrap-up","title":"\ud83c\udfc1 Sprint Wrap-up","text":""},{"location":"sprint-summaries/sprint2/#what-has-been-done","title":"What has been done","text":"<ul> <li>Backend connected to the database.</li> <li>Frontend fetching data from API endpoints.</li> <li>3 proposals of architecture</li> <li>MkDocs documentation structure created and running locally.</li> </ul>"},{"location":"sprint-summaries/sprint2/#what-went-well","title":"What went well","text":""},{"location":"sprint-summaries/sprint2/#what-can-we-do-better","title":"What can we do better","text":"<ul> <li>Improve coordination and communication across subteams.</li> <li>Keep ClickUp board updated more frequently.</li> <li>Divide work inside subteams</li> </ul>"},{"location":"sprint-summaries/sprint2/#next-sprint-2710-1311","title":"\ud83d\ude80 Next Sprint (27/10 \u2013 13/11)","text":""},{"location":"sprint-summaries/sprint2/#what-should-be-done","title":"What should be done","text":"<ul> <li>Frontend and Backend should decide about what data that should be visualized<ul> <li>Ask Client about their interest in already explored data </li> </ul> </li> <li>UX Mockup uploaded to ClickUp and/or GitHub</li> <li>Data extraction team:<ul> <li>Identify viable data that indicates the state of the machine (on/off/etc.)<ul> <li>Find/create time intervals when the machine is on<ul> <li>Extract specific variables during these intervals</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"sprint-summaries/sprint2/#documentation","title":"Documentation","text":"<ul> <li>Add separate documentation sections for:</li> <li>Backend </li> <li>Frontend </li> <li>Analytics/Visualization (KPIs maybe)</li> <li>Ensure each teammate updates relevant documentation <code>.md</code> files as their part evolves.</li> </ul>"},{"location":"sprint-summaries/sprint2/#user-stories","title":"User Stories","text":"<ul> <li>Add or refine new user stories in ClickUp for this sprint.</li> <li>Each story should have:</li> <li>A clear goal</li> <li>Acceptance criteria</li> <li>Assigned responsible member(s)</li> </ul>"},{"location":"sprint-summaries/sprint3/","title":"Sprint 3 Summary (27/10 \u2013 13/11)","text":""},{"location":"sprint-summaries/sprint3/#sprint-wrap-up","title":"\ud83c\udfc1 Sprint Wrap-up","text":""},{"location":"sprint-summaries/sprint3/#what-has-been-done","title":"What has been done","text":""},{"location":"sprint-summaries/sprint3/#-ux-mockup-presented","title":"- UX Mockup presented","text":""},{"location":"sprint-summaries/sprint3/#what-went-well","title":"What went well","text":"<ul> <li>Great UX mockup for a visionary of MVP</li> </ul>"},{"location":"sprint-summaries/sprint3/#what-can-we-do-better","title":"What can we do better","text":"<ul> <li>Keep improve coordination and communication across subteams.</li> <li>Keep ClickUp board updated (ALL TEAMS)</li> <li>More code review (remember DoD)</li> <li>More meetings</li> <li>Better communication in channels (WhatsApp)</li> </ul>"},{"location":"sprint-summaries/sprint3/#next-sprint-1411-2411","title":"\ud83d\ude80 Next Sprint (14/11 \u2013 24/11)","text":""},{"location":"sprint-summaries/sprint3/#what-should-be-done","title":"What should be done","text":"<ul> <li>Ask Client about opinion nn already explored data </li> <li>Extraction team: Explore variables related to issues with machinery</li> <li>Frontend team: Work on visualization for example graphs of temperature</li> <li>Document (in python and .md files) what has already been done in</li> <li>Extraction</li> <li>Backend </li> <li>Frontend </li> <li>Remove hardcoding of configuration parameters<ul> <li>Use yaml / toml syntax for those files</li> <li>Tutorial slides from lecture T04 called \"Practical Notes\"</li> </ul> </li> <li>Identify viable data that indicates the state of the machine (on/off/etc.)</li> </ul>"},{"location":"sprint-summaries/sprint3/#documentation","title":"Documentation","text":"<ul> <li>Finish mkdocs documentation guide</li> </ul>"},{"location":"sprint-summaries/sprint3/#user-stories","title":"User Stories","text":"<ul> <li>Add or refine new user stories in ClickUp for this sprint.</li> <li>Each story should have:</li> <li>A clear goal</li> <li>Acceptance criteria</li> <li>Assigned responsible member(s)</li> </ul>"}]}