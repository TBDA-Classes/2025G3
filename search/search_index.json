{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"architecture/","title":"\ud83e\uddf1 Architectural Evolution \u2013 CNC Data Analytics System","text":""},{"location":"architecture/#preliminary-note","title":"\u26a0\ufe0f Preliminary Note","text":"<p>The following architectural proposals (Architectures A, B, and C) have been initially designed with the support of ChatGPT (OpenAI) as a reference framework for the project \u201cUnderstanding from Numerical Control Driven Machining Control.\u201d</p> <p>They represent conceptual drafts and working hypotheses, not final decisions. All elements \u2014 including component choices, data flows, and execution layers \u2014 must be reviewed, discussed, and validated by the entire project team before implementation. Any modification, simplification, or extension should be agreed upon collectively to ensure technical consistency and feasibility.</p>"},{"location":"architecture/#architecture-a-local-batch-pipeline","title":"\u2699\ufe0f Architecture A \u2014 \u201cLocal Batch Pipeline\u201d","text":""},{"location":"architecture/#concept","title":"\ud83d\udd39 Concept","text":"<p>A simple, local-first architecture for initial experimentation. All processing is performed on the user\u2019s PC, while data are stored on a remote PostgreSQL database. It enables the team to explore the dataset, understand machine behavior, and prototype the analytics pipeline.  </p> <p>This is the starting point for the project \u2014 low complexity, minimal infrastructure, and ideal for the early development phase.</p>"},{"location":"architecture/#main-components","title":"\ud83e\udde9 Main Components","text":"Layer Component Execution / Hosting Description Production plant CNC Machine Physical system Produces raw operational variables (axis position, spindle load, etc.) Cloud / Server PostgreSQL Database (raw data) Hosted on UPM / remote server Stores high-frequency machine signals Local PC Python ETL (pandas, SQL) Executed locally in Jupyter / Spyder Extracts and cleans data, performs simple aggregations Python Analytics Executed locally in Jupyter / Spyder Computes KPIs, operation time, energy per program, and alerts Streamlit Dashboard (UI) Executed locally with Streamlit (Python web server) Visualizes results and provides basic user interaction User Operator / Analyst Local access Explores data, applies filters, interprets results"},{"location":"architecture/#workflow","title":"\ud83d\udd04 Workflow","text":"<ol> <li>The CNC machine sends raw variables to the PostgreSQL database.  </li> <li>Local ETL scripts retrieve and clean the data.  </li> <li>Analytics modules compute machine indicators and alerts.  </li> <li>The dashboard presents results interactively.</li> </ol>"},{"location":"architecture/#advantages","title":"\u2705 Advantages","text":"<ul> <li>Very simple to deploy and maintain  </li> <li>Ideal for small datasets or offline analysis  </li> <li>Full transparency and control for debugging and learning  </li> </ul>"},{"location":"architecture/#limitations","title":"\u26a0\ufe0f Limitations","text":"<ul> <li>Query time increases with dataset size  </li> <li>Manual updates required (no automation)  </li> <li>No real-time feedback; purely batch operation  </li> </ul>"},{"location":"architecture/#adoption-timeline","title":"\ud83d\udd52 Adoption timeline","text":"<p>Used in Phase 1 (project start). Goal: validate the data model, ensure database connectivity, and develop the first analysis scripts.</p>"},{"location":"architecture/#architecture-diagram","title":"\ud83d\udcf7 Architecture Diagram","text":""},{"location":"architecture/#architecture-b-optimized-time-series-pipeline","title":"\u26a1 Architecture B \u2014 \u201cOptimized Time-Series Pipeline\u201d","text":""},{"location":"architecture/#concept_1","title":"\ud83d\udd39 Concept","text":"<p>An upgraded version focusing on performance and scalability. PostgreSQL is extended with TimescaleDB, enabling hypertables and continuous aggregation for fast time-based queries. This allows the system to handle larger volumes of CNC data with near-real-time responsiveness.</p>"},{"location":"architecture/#main-components_1","title":"\ud83e\udde9 Main Components","text":"Layer Component Execution / Hosting Description Production plant CNC Machine Physical source Streams operational data continuously Cloud / Server PostgreSQL (raw data) Hosted on remote server Stores unprocessed variables TimescaleDB Hosted on cloud server \u2013 automatic aggregation Extends PostgreSQL to manage time-series efficiently Local PC Python ETL Executed locally in Jupyter / Spyder Loads data, applies additional transformations and cleaning Python Analytics Executed locally in Jupyter / Spyder Performs KPI calculation, energy/time estimation, and alert generation Streamlit Dashboard (UI) Executed locally with Streamlit Visualizes aggregated data with time filters and metrics User Operator / Analyst Local or LAN access Uses the dashboard for insights and comparisons"},{"location":"architecture/#workflow_1","title":"\ud83d\udd04 Workflow","text":"<ol> <li>The CNC data are inserted into PostgreSQL (raw).  </li> <li>TimescaleDB automatically builds hypertables and continuous aggregates.  </li> <li>Local ETL scripts access pre-aggregated data instead of raw tables.  </li> <li>Analytics compute KPIs and alerts efficiently.  </li> <li>The dashboard displays up-to-date results.</li> </ol>"},{"location":"architecture/#advantages_1","title":"\u2705 Advantages","text":"<ul> <li>Fast queries thanks to TimescaleDB hypertables  </li> <li>Continuous aggregation handled automatically on the server  </li> <li>Scalable to millions of records  </li> <li>Compatible with the same local Python workflow  </li> </ul>"},{"location":"architecture/#limitations_1","title":"\u26a0\ufe0f Limitations","text":"<ul> <li>Requires server configuration for TimescaleDB  </li> <li>Still partially manual (batch updates)  </li> <li>No true real-time streaming yet  </li> </ul>"},{"location":"architecture/#adoption-timeline_1","title":"\ud83d\udd52 Adoption timeline","text":"<p>Used in Phase 2 (mid-project). Goal: optimize performance and manage higher data volumes once the prototype is validated.</p>"},{"location":"architecture/#architecture-diagram_1","title":"\ud83d\udcf7 Architecture Diagram","text":""},{"location":"architecture/#architecture-c-streaming-real-time-pipeline","title":"\ud83c\udf10 Architecture C \u2014 \u201cStreaming / Real-Time Pipeline\u201d","text":""},{"location":"architecture/#concept_2","title":"\ud83d\udd39 Concept","text":"<p>The most advanced version, enabling real-time monitoring and analytics. This architecture integrates Apache Kafka for data streaming and Spark Structured Streaming for on-the-fly processing. It combines real-time pipelines with long-term storage and visualization.</p>"},{"location":"architecture/#main-components_2","title":"\ud83e\udde9 Main Components","text":"Layer Component Execution / Hosting Description Production plant CNC Machine Physical source Continuously generates real-time signals Cloud / Processing layer Apache Kafka (stream) Hosted on cloud server \u2013 streaming message broker Receives live data, buffers, and distributes messages Spark Structured Streaming Executed on cloud server \u2013 real-time processing engine Processes Kafka streams, detects patterns and alerts, forwards results PostgreSQL (operational storage) Hosted on remote server Stores latest operational data for fast queries TimescaleDB (historical storage) Hosted on remote server Maintains long-term time-series logs and aggregates Local PC Python ETL Executed locally in Jupyter / Spyder Periodically integrates cloud data for additional analysis Python Analytics Executed locally in Jupyter / Spyder Further exploration and validation of real-time results Streamlit Dashboard (UI) Executed locally with Streamlit Displays live machine state, alerts, and KPIs in near real-time User Operator / Analyst Web access Monitors operations and system health interactively"},{"location":"architecture/#workflow_2","title":"\ud83d\udd04 Workflow","text":"<ol> <li>CNC sensors send live data streams to Apache Kafka.  </li> <li>Kafka buffers and forwards events to Spark Structured Streaming.  </li> <li>Spark aggregates and processes data in micro-batches, detecting operation cycles and anomalies.  </li> <li>Results are written to PostgreSQL (for fast querying) and TimescaleDB (for historical logs).  </li> <li>The Streamlit dashboard continuously updates with live KPIs and alerts.</li> </ol>"},{"location":"architecture/#advantages_2","title":"\u2705 Advantages","text":"<ul> <li>Real-time data ingestion and processing  </li> <li>Immediate alerting and visualization  </li> <li>Hybrid storage (short-term + long-term)  </li> <li>Scalable to industrial workloads  </li> </ul>"},{"location":"architecture/#limitations_2","title":"\u26a0\ufe0f Limitations","text":"<ul> <li>Higher infrastructure complexity  </li> <li>Requires Kafka and Spark deployment  </li> <li>Needs reliable network and cloud configuration  </li> </ul>"},{"location":"architecture/#adoption-timeline_2","title":"\ud83d\udd52 Adoption timeline","text":"<p>Used in Phase 3 (final stage). Goal: demonstrate real-time analytics capability, provide continuous monitoring, and connect the full data lifecycle (machine \u2192 cloud \u2192 user).</p>"},{"location":"architecture/#architecture-diagram_2","title":"\ud83d\udcf7 Architecture Diagram","text":""},{"location":"architecture/#overall-evolution-summary","title":"\ud83e\udded Overall Evolution Summary","text":"Stage Main Goal Key Technologies Deployment Level A \u2013 Local Batch Prototype, data understanding PostgreSQL, Python (pandas), Streamlit Local execution B \u2013 Time-Series Optimized Performance &amp; scalability TimescaleDB, Python (ETL/Analytics) Local + Cloud C \u2013 Streaming / Real-Time Real-time insights &amp; automation Kafka, Spark, PostgreSQL, TimescaleDB Cloud + Local UI"},{"location":"architecture/#progressive-implementation-strategy","title":"\ud83d\udd04 Progressive Implementation Strategy","text":"<ol> <li>Phase 1: Start with Architecture A \u2192 focus on ETL pipeline, KPIs, and dashboard.  </li> <li>Phase 2: Migrate to Architecture B \u2192 activate TimescaleDB and optimize queries.  </li> <li>Phase 3: Extend to Architecture C \u2192 integrate streaming for real-time feedback and alerts.</li> </ol> <p>This progressive approach ensures that: - The team learns the full data lifecycle step by step. - Each phase delivers a working, demonstrable system. - The final architecture (C) aligns with modern Industry 4.0 and IoT paradigms \u2014 turning raw CNC signals into real-time, actionable intelligence.</p>"},{"location":"overview/","title":"Documentation overview","text":""},{"location":"overview/#1-purpose","title":"1. Purpose","text":"<p>This folder contains (should contain) all technical and client-facing documentation for the project. The goal is to keep the documentation consistent and easy to navigate, across all subteams.</p>"},{"location":"overview/#2-structure","title":"2. Structure","text":"File / Folder Description overview.md You are here \u2014 index of all documentation files. architecture.md System design, data flow, and integration between backend, data, and UI backend.md Backend API endpoints, database connection details ui_visualization.md Frontend/dashboard structure, visualization logic, and dependencies. client/ Client-facing documentation: user manual, demo guide etc. sprint_summaries/ Sprint-by-sprint summaries and notes."},{"location":"overview/#3-documentation-guidelines","title":"3. Documentation Guidelines","text":"<p>To keep documentation consistent and easy to navigate, every subteam should:  </p> <ol> <li>Update documentation alongside code changes. </li> <li>When you finish a new feature, update the relevant <code>.md</code> file in this folder.  </li> <li>Follow consistent formatting. </li> <li>Use clear section headers (<code>##</code>), bullet points, and short paragraphs.   </li> <li>Avoid duplication. </li> <li>Link to other files instead of rewriting the same content. Example:      See architecture.md </li> </ol>"},{"location":"backend/api/","title":"Backend Overview","text":"<ul> <li>For now this document acts as a placeholder</li> <li>This section describes the backend API, endpoints, and database connections etc.</li> </ul>"},{"location":"frontend/ui/","title":"Frontend Overview","text":"<p>This section describes the frontend architecture, mockups, and components</p>"},{"location":"frontend/ui/#ux-mockup","title":"UX Mockup","text":""},{"location":"sprint-summaries/sprint1/","title":"Sprint 1 Summary (30/09 \u2013 13/10)","text":""},{"location":"sprint-summaries/sprint1/#sprint-wrap-up","title":"\ud83c\udfc1 Sprint Wrap-up","text":""},{"location":"sprint-summaries/sprint1/#what-has-been-done","title":"What has been done","text":"<ul> <li>Project team organized according to Scrum methodology (roles, subteams, tools defined).</li> <li>Repository and ClickUp workspace created and configured.</li> <li>Initial meetings with the client to understand goals and available data.</li> <li>First exploration of datasets shared by the client.</li> <li>Project architecture brainstorming and first drafts prepared.</li> </ul>"},{"location":"sprint-summaries/sprint1/#what-went-well","title":"What went well","text":"<ul> <li>Team adapted to collaborative tools (GitHub, ClickUp)</li> </ul>"},{"location":"sprint-summaries/sprint1/#what-can-we-do-better","title":"What can we do better","text":"<ul> <li>Improve documentation frequency.</li> <li>Maintain updated ClickUp tasks to track progress more effectively.</li> <li>Ensure all members are aligned on the data understanding phase.</li> </ul>"},{"location":"sprint-summaries/sprint1/#next-sprint-1310-2610","title":"\ud83d\ude80 Next Sprint (13/10 \u2013 26/10)","text":""},{"location":"sprint-summaries/sprint1/#what-should-be-done","title":"What should be done","text":"<ul> <li>Backend connection to the database.</li> <li>Frontend starts interacting with API endpoints.</li> <li>Define and evaluate alternative architecture proposals.</li> <li>Continue dataset cleaning and understanding.</li> </ul>"},{"location":"sprint-summaries/sprint1/#documentation","title":"Documentation","text":"<ul> <li>Create dedicated <code>.md</code> files for backend, frontend, and data analysis.</li> <li>Upload diagrams or drafts related to the three architecture proposals.</li> </ul>"},{"location":"sprint-summaries/sprint1/#user-stories","title":"User Stories","text":"<ul> <li>Refine initial user stories and link them to corresponding tasks in ClickUp.</li> <li>Define clear acceptance criteria for technical and documentation deliverables.</li> </ul>"},{"location":"sprint-summaries/sprint2/","title":"Sprint 2 Summary (13/10 \u2013 26/10)","text":""},{"location":"sprint-summaries/sprint2/#sprint-wrap-up","title":"\ud83c\udfc1 Sprint Wrap-up","text":""},{"location":"sprint-summaries/sprint2/#what-has-been-done","title":"What has been done","text":"<ul> <li>Backend connected to the database.</li> <li>Frontend fetching data from API endpoints.</li> <li>3 proposals of architecture</li> <li>MkDocs documentation structure created and running locally.</li> </ul>"},{"location":"sprint-summaries/sprint2/#what-went-well","title":"What went well","text":""},{"location":"sprint-summaries/sprint2/#what-can-we-do-better","title":"What can we do better","text":"<ul> <li>Improve coordination and communication across subteams.</li> <li>Keep ClickUp board updated more frequently.</li> <li>Divide work inside subteams</li> </ul>"},{"location":"sprint-summaries/sprint2/#next-sprint-2710-1311","title":"\ud83d\ude80 Next Sprint (27/10 \u2013 13/11)","text":""},{"location":"sprint-summaries/sprint2/#what-should-be-done","title":"What should be done","text":"<ul> <li>Frontend and Backend should decide about what data that should be visualized<ul> <li>Ask Client about their interest in already explored data </li> </ul> </li> <li>UX Mockup uploaded to ClickUp and/or GitHub</li> <li>Data extraction team:<ul> <li>Identify viable data that indicates the state of the machine (on/off/etc.)<ul> <li>Find/create time intervals when the machine is on<ul> <li>Extract specific variables during these intervals</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"sprint-summaries/sprint2/#documentation","title":"Documentation","text":"<ul> <li>Add separate documentation sections for:</li> <li>Backend </li> <li>Frontend </li> <li>Analytics/Visualization (KPIs maybe)</li> <li>Ensure each teammate updates relevant documentation <code>.md</code> files as their part evolves.</li> </ul>"},{"location":"sprint-summaries/sprint2/#user-stories","title":"User Stories","text":"<ul> <li>Add or refine new user stories in ClickUp for this sprint.</li> <li>Each story should have:</li> <li>A clear goal</li> <li>Acceptance criteria</li> <li>Assigned responsible member(s)</li> </ul>"},{"location":"sprint-summaries/sprint3/","title":"Sprint 3 Summary (27/10 \u2013 13/11)","text":""},{"location":"sprint-summaries/sprint3/#sprint-wrap-up","title":"\ud83c\udfc1 Sprint Wrap-up","text":""},{"location":"sprint-summaries/sprint3/#what-has-been-done","title":"What has been done","text":""},{"location":"sprint-summaries/sprint3/#-ux-mockup-presented","title":"- UX Mockup presented","text":""},{"location":"sprint-summaries/sprint3/#what-went-well","title":"What went well","text":"<ul> <li>Great UX mockup for a visionary of MVP</li> </ul>"},{"location":"sprint-summaries/sprint3/#what-can-we-do-better","title":"What can we do better","text":"<ul> <li>Keep improve coordination and communication across subteams.</li> <li>Keep ClickUp board updated (ALL TEAMS)</li> <li>More code review (remember DoD)</li> <li>More meetings</li> <li>Better communication in channels (WhatsApp)</li> </ul>"},{"location":"sprint-summaries/sprint3/#next-sprint-1411-2411","title":"\ud83d\ude80 Next Sprint (14/11 \u2013 24/11)","text":""},{"location":"sprint-summaries/sprint3/#what-should-be-done","title":"What should be done","text":"<ul> <li>Ask Client about opinion nn already explored data </li> <li>Extraction team: Explore variables related to issues with machinery</li> <li>Frontend team: Work on visualization for example graphs of temperature</li> <li>Document (in python and .md files) what has already been done in</li> <li>Extraction</li> <li>Backend </li> <li>Frontend </li> <li>Remove hardcoding of configuration parameters<ul> <li>Use yaml / toml syntax for those files</li> <li>Tutorial slides from lecture T04 called \"Practical Notes\"</li> </ul> </li> <li>Identify viable data that indicates the state of the machine (on/off/etc.)</li> </ul>"},{"location":"sprint-summaries/sprint3/#documentation","title":"Documentation","text":"<ul> <li>Finish mkdocs documentation guide</li> </ul>"},{"location":"sprint-summaries/sprint3/#user-stories","title":"User Stories","text":"<ul> <li>Add or refine new user stories in ClickUp for this sprint.</li> <li>Each story should have:</li> <li>A clear goal</li> <li>Acceptance criteria</li> <li>Assigned responsible member(s)</li> </ul>"}]}